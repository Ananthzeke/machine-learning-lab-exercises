{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Classification Algorithms (BBNN & SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Back Propogation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        contents = yaml.full_load(f)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPNN:\n",
    "    def error(self,predicted, actual):\n",
    "        error = actual-predicted\n",
    "        return error @ error\n",
    "    \n",
    "    def transfer(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def transfer_derivative(self,x):\n",
    "         return x * (1 - x)\n",
    "    \n",
    "    def set_random_weights(self, neuron_ns,seed=1):\n",
    "        self.n = len(neuron_ns)-1\n",
    "        np.random.seed(seed)\n",
    "        self.weights = [\n",
    "            np.random.random(((neuron_ns[i+1], neuron_ns[i]+1)))\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "        self.outputs = [np.zeros((size+1)) for size in neuron_ns]\n",
    "        self.delta = [np.zeros((size+1)) for size in neuron_ns]\n",
    "        for x in self.outputs:\n",
    "            x[0] = 1\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        assert (self.weights is not None),\"weights not given\"\n",
    "        self.outputs[0][1:] = inputs\n",
    "        for i in range(self.n):\n",
    "            self.outputs[i+1][1:] = self.transfer(\n",
    "                self.weights[i] @ self.outputs[i]\n",
    "            )\n",
    "        return self.outputs[self.n][1:]\n",
    "\n",
    "    def backward_propagate_error(self, expected):\n",
    "        for i in range(self.n, 0, -1):\n",
    "            if i == self.n:\n",
    "                errors = expected - self.outputs[i][1:]\n",
    "            else:\n",
    "                errors = self.weights[i].T[1:] @ self.delta[i+1][1:]\n",
    "            self.delta[i][1:] = errors * \\\n",
    "                self.transfer_derivative(self.outputs[i][1:])\n",
    "\n",
    "    def update_weights(self, l_rate):\n",
    "        for i in range(self.n):\n",
    "            self.weights[i] += l_rate * \\\n",
    "                self.delta[i+1][1:, np.newaxis] @ self.outputs[i][np.newaxis, :]\n",
    "\n",
    "    def fit(self, train_x,train_y, l_rate, n_epoch):\n",
    "        classes = np.unique(train_y)\n",
    "        expected = (train_y.reshape(-1,1) == classes).astype(np.uint8)\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0\n",
    "            for i, row in enumerate(train_x):\n",
    "                outputs = self.forward_propagate(row)\n",
    "                sum_error += self.error(outputs, expected[i])\n",
    "                self.backward_propagate_error(expected[i])\n",
    "                self.update_weights(l_rate)\n",
    "            print(f'>epoch={epoch}, lrate={l_rate:.3},'\n",
    "                  f' error={sum_error:.4}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        outputs = self.forward_propagate(inputs).argmax()\n",
    "        return outputs\n",
    "\n",
    "    def validate(self,x_test,y_test):\n",
    "        m = x_test.shape[0]\n",
    "        for i in range(m):\n",
    "            actual = self.predict(x_test[i])\n",
    "            expected = y_test[i].astype(int)\n",
    "            print(f\"Expected={expected}, Got={actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "\n",
      "[[ 2.7810836   2.550537    0.        ]\n",
      " [ 1.46548937  2.36212508  0.        ]\n",
      " [ 3.39656169  4.40029353  0.        ]\n",
      " [ 1.38807019  1.85022032  0.        ]\n",
      " [ 3.06407232  3.00530597  0.        ]\n",
      " [ 7.62753121  2.75926224  1.        ]\n",
      " [ 5.33244125  2.08862677  1.        ]\n",
      " [ 6.92259672  1.77106367  1.        ]\n",
      " [ 8.67541865 -0.24206865  1.        ]\n",
      " [ 7.67375647  3.50856301  1.        ]\n",
      " [ 2.7810836   2.550537    0.        ]\n",
      " [ 1.46548937  2.36212508  0.        ]\n",
      " [ 3.39656169  4.40029353  0.        ]\n",
      " [ 1.38807019  1.85022032  0.        ]\n",
      " [ 3.06407232  3.00530597  0.        ]\n",
      " [ 7.62753121  2.75926224  1.        ]\n",
      " [ 5.33244125  2.08862677  1.        ]\n",
      " [ 6.92259672  1.77106367  1.        ]\n",
      " [ 8.67541865 -0.24206865  1.        ]\n",
      " [ 7.67375647  3.50856301  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(load_yaml(\"./datasets/bpnn_dataset.yaml\"))\n",
    "print(\"Dataset:\\n\",dataset,sep=\"\\n\")\n",
    "split_ratio = 0.5\n",
    "split = int(split_ratio*dataset.shape[0])\n",
    "train, test = dataset[:split],dataset[split:]\n",
    "train_x, train_y = train[:,:-1], train[:,-1]\n",
    "test_x, test_y = test[:,:-1], test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Weights:\n",
      "\n",
      "[[0.15416284 0.7400497  0.26331502]\n",
      " [0.53373939 0.01457496 0.91874701]]\n",
      "[[0.90071485 0.03342143 0.95694934]\n",
      " [0.13720932 0.28382835 0.60608318]]\n",
      "\n",
      "Training:\n",
      "\n",
      ">epoch=0, lrate=0.5, error=6.465\n",
      ">epoch=1, lrate=0.5, error=5.533\n",
      ">epoch=2, lrate=0.5, error=4.924\n",
      ">epoch=3, lrate=0.5, error=4.59\n",
      ">epoch=4, lrate=0.5, error=4.304\n",
      ">epoch=5, lrate=0.5, error=3.984\n",
      ">epoch=6, lrate=0.5, error=3.635\n",
      ">epoch=7, lrate=0.5, error=3.267\n",
      ">epoch=8, lrate=0.5, error=2.888\n",
      ">epoch=9, lrate=0.5, error=2.519\n",
      ">epoch=10, lrate=0.5, error=2.187\n",
      ">epoch=11, lrate=0.5, error=1.898\n",
      ">epoch=12, lrate=0.5, error=1.65\n",
      ">epoch=13, lrate=0.5, error=1.438\n",
      ">epoch=14, lrate=0.5, error=1.259\n",
      ">epoch=15, lrate=0.5, error=1.108\n",
      ">epoch=16, lrate=0.5, error=0.9817\n",
      ">epoch=17, lrate=0.5, error=0.8753\n",
      ">epoch=18, lrate=0.5, error=0.7854\n",
      ">epoch=19, lrate=0.5, error=0.7092\n",
      "\n",
      "Trained Weights:\n",
      "\n",
      "[[-0.31419445  0.86239965 -1.05449705]\n",
      " [ 0.82372542 -1.34886773  1.78507267]]\n",
      "[[-0.1445459  -1.68269707  2.3485556 ]\n",
      " [ 0.23456386  1.40676681 -2.12961966]]\n",
      "\n",
      "Validation:\n",
      "\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "b = BPNN()\n",
    "b.set_random_weights([2,2,2],12)\n",
    "print(\"\\nInitial Weights:\\n\")\n",
    "print(*b.weights,sep=\"\\n\")\n",
    "print(\"\\nTraining:\\n\")\n",
    "b.fit(train_x,train_y,.5, 20)\n",
    "print(\"\\nTrained Weights:\\n\")\n",
    "print(*b.weights,sep=\"\\n\")\n",
    "print(\"\\nValidation:\\n\")\n",
    "b.validate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from cvxopt import solvers\n",
    "from cvxopt import matrix\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# RBF KERNEL:\n",
    "def getRBFKernelMatrix(X1,X2,sigma):\n",
    "    return np.exp(-cdist(X1, X2, 'sqeuclidean') / (2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed titanic dataset from Exercise 2\n",
    "titanic_df = pd.read_csv('datasets/titanic_processed.csv')\n",
    "X = titanic_df.drop('Survived', 1).values\n",
    "y = titanic_df['Survived'].values\n",
    "y[y==0] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.4858e+01 -1.0107e+03  3e+03  1e+00  1e-15\n",
      " 1: -7.6163e+01 -4.6417e+02  4e+02  9e-03  9e-16\n",
      " 2: -9.2205e+01 -1.3153e+02  4e+01  8e-04  1e-15\n",
      " 3: -9.6445e+01 -1.0501e+02  9e+00  1e-04  7e-16\n",
      " 4: -9.7370e+01 -1.0041e+02  3e+00  3e-05  7e-16\n",
      " 5: -9.7740e+01 -9.9065e+01  1e+00  5e-15  7e-16\n",
      " 6: -9.7873e+01 -9.8182e+01  3e-01  2e-15  7e-16\n",
      " 7: -9.7908e+01 -9.8030e+01  1e-01  9e-16  7e-16\n",
      " 8: -9.7919e+01 -9.7984e+01  7e-02  2e-15  7e-16\n",
      " 9: -9.7928e+01 -9.7949e+01  2e-02  8e-16  7e-16\n",
      "10: -9.7930e+01 -9.7943e+01  1e-02  3e-15  7e-16\n",
      "11: -9.7933e+01 -9.7934e+01  1e-03  2e-15  7e-16\n",
      "12: -9.7933e+01 -9.7933e+01  7e-05  5e-15  7e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "sigma=.5\n",
    "C=1\n",
    "n=X_train.shape[0]\n",
    "I_n = np.eye(n)\n",
    "P=getRBFKernelMatrix(X_train,X_train,sigma)\n",
    "q=np.full(n,-1)\n",
    "G=np.vstack((I_n,-1*I_n))\n",
    "h=np.hstack((np.full(n,C),np.zeros(n)))\n",
    "A=y_train.reshape(1,-1)\n",
    "b=np.zeros(1)\n",
    "\n",
    "P,q,G,h,A,b = map(lambda x : matrix(x,tc=\"d\"),(P,q,G,h,A,b))\n",
    "\n",
    "a = solvers.qp(P, q, G, h, A, b)\n",
    "a = np.asarray(a['x']).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.66\n"
     ]
    }
   ],
   "source": [
    "idx = np.logical_and(a>=1e-10,  a<C)\n",
    "Xy = X_train * y_train.reshape(-1,1)\n",
    "w0 = np.mean((y_train - getRBFKernelMatrix(Xy,X_train,sigma) @ a)[idx] )\n",
    "y_pred = np.sign(getRBFKernelMatrix(X_test,Xy,sigma).dot(a)+w0)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand {\\norm}[1] {\\lVert{#1}\\rVert}$$\n",
    "$$\n",
    "\\min \\frac{1}{2}\\norm{w}_2^2+C\\sum_{i=1}^{n} \\xi_i\\\\\n",
    "s.t. y_i(w^Tx_i−b)≥1−\\xi i,\\xi i≥0,∀i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\norm{W}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('test': conda)",
   "language": "python",
   "name": "python38364bittestcondaeacc9aeeb2cc47fcad4d66ae875066bb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
